"""
Keyword Discovery Tool
ë°ì´í„°ë¡œë¶€í„° ì‹¤ì œ í‚¤ì›Œë“œë¥¼ ë°œê²¬í•˜ëŠ” ë„êµ¬
"""

import pandas as pd
from collections import Counter
import re
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np


class KeywordDiscovery:
    def __init__(self, csv_path):
        print("Loading dataset...")
        self.df = pd.read_csv(csv_path)

        # Rating groups
        self.low_rating = self.df[self.df['rating'] <= 2]
        self.mid_rating = self.df[self.df['rating'] == 3]
        self.high_rating = self.df[self.df['rating'] >= 4]

        print(f"Loaded {len(self.df)} reviews")
        print(f"Low rating: {len(self.low_rating)}, Mid: {len(self.mid_rating)}, High: {len(self.high_rating)}")

    def extract_frequent_words(self, rating_group, n=100):
        """
        ë°©ë²• 1: ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        ê°€ì¥ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ ì°¾ê¸°
        """
        if rating_group == 'low':
            reviews = self.low_rating['review_cleaned'].fillna('')
        elif rating_group == 'mid':
            reviews = self.mid_rating['review_cleaned'].fillna('')
        else:
            reviews = self.high_rating['review_cleaned'].fillna('')

        # ëª¨ë“  ë¦¬ë·° í•©ì¹˜ê¸°
        all_text = ' '.join(reviews.astype(str))

        # ë‹¨ì–´ ì¶”ì¶œ (3ê¸€ì ì´ìƒ)
        words = re.findall(r'\b[a-z]{3,}\b', all_text.lower())

        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {
            'the', 'and', 'for', 'with', 'this', 'that', 'from', 'have', 'has',
            'was', 'were', 'are', 'app', 'one', 'can', 'but', 'not', 'get',
            'use', 'like', 'would', 'even', 'just', 'really', 'also', 'much',
            'make', 'very', 'still', 'more', 'its', 'been', 'had', 'will',
            'all', 'you', 'your', 'only', 'need', 'want', 'than', 'way',
            'could', 'when', 'there', 'what', 'which', 'their', 'they',
            'some', 'out', 'into', 'about', 'then', 'than', 'over', 'back'
        }

        word_freq = Counter([w for w in words if w not in stop_words])

        return dict(word_freq.most_common(n))

    def extract_distinctive_words_tfidf(self, rating_group, n=50):
        """
        ë°©ë²• 2: TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        í•´ë‹¹ ê·¸ë£¹ì—ì„œ "íŠ¹ì§•ì ìœ¼ë¡œ" ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì°¾ê¸°
        """
        # ê° ê·¸ë£¹ë³„ í…ìŠ¤íŠ¸ ì¤€ë¹„
        low_text = ' '.join(self.low_rating['review_cleaned'].fillna('').astype(str))
        mid_text = ' '.join(self.mid_rating['review_cleaned'].fillna('').astype(str))
        high_text = ' '.join(self.high_rating['review_cleaned'].fillna('').astype(str))

        documents = [low_text, mid_text, high_text]
        doc_names = ['low_rating', 'mid_rating', 'high_rating']

        # TF-IDF ê³„ì‚°
        vectorizer = TfidfVectorizer(
            max_features=200,
            stop_words='english',
            ngram_range=(1, 2),  # 1-gramê³¼ 2-gram ëª¨ë‘
            min_df=2
        )

        tfidf_matrix = vectorizer.fit_transform(documents)
        feature_names = vectorizer.get_feature_names_out()

        # í•´ë‹¹ ê·¸ë£¹ì˜ ì¸ë±ìŠ¤
        group_idx = {'low': 0, 'mid': 1, 'high': 2}[rating_group]

        # TF-IDF ì ìˆ˜ ì¶”ì¶œ
        scores = tfidf_matrix[group_idx].toarray()[0]

        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        top_indices = scores.argsort()[-n:][::-1]
        top_words = [(feature_names[i], scores[i]) for i in top_indices]

        return top_words

    def extract_ngrams(self, rating_group, n=2, top_k=50):
        """
        ë°©ë²• 3: N-gram ë¶„ì„
        ìì£¼ í•¨ê»˜ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ ì¡°í•© ì°¾ê¸° (ì˜ˆ: "data loss", "premium features")
        """
        if rating_group == 'low':
            reviews = self.low_rating['review_cleaned'].fillna('')
        elif rating_group == 'mid':
            reviews = self.mid_rating['review_cleaned'].fillna('')
        else:
            reviews = self.high_rating['review_cleaned'].fillna('')

        all_text = ' '.join(reviews.astype(str))

        # N-gram ì¶”ì¶œ
        words = all_text.lower().split()
        ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]

        # ë¹ˆë„ ê³„ì‚°
        ngram_freq = Counter(ngrams)

        return dict(ngram_freq.most_common(top_k))

    def compare_groups(self, top_n=30):
        """
        ë°©ë²• 4: ê·¸ë£¹ ê°„ ë¹„êµ ë¶„ì„
        ì €í‰ì ì—ëŠ” ë§ê³  ê³ í‰ì ì—ëŠ” ì ì€ ë‹¨ì–´ ì°¾ê¸°
        """
        low_words = self.extract_frequent_words('low', n=500)
        high_words = self.extract_frequent_words('high', n=500)

        # ì €í‰ì ì—ì„œë§Œ ë‘ë“œëŸ¬ì§€ëŠ” ë‹¨ì–´
        distinctive_low = {}
        for word, low_count in low_words.items():
            high_count = high_words.get(word, 0)

            # ì €í‰ì ì—ì„œ í›¨ì”¬ ë§ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´
            if low_count > 100 and (low_count / (high_count + 1)) > 2:
                distinctive_low[word] = {
                    'low_count': low_count,
                    'high_count': high_count,
                    'ratio': low_count / (high_count + 1)
                }

        # ë¹„ìœ¨ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_words = dict(sorted(distinctive_low.items(),
                                  key=lambda x: x[1]['ratio'],
                                  reverse=True)[:top_n])

        return sorted_words

    def discover_pain_point_keywords(self):
        """
        Pain Point í‚¤ì›Œë“œ ìë™ ë°œê²¬
        """
        print("\n" + "="*80)
        print("PAIN POINT KEYWORD DISCOVERY (from 1-2 star reviews)")
        print("="*80)

        # ë°©ë²• 1: ë¹ˆë„ ë¶„ì„
        print("\n1ï¸âƒ£  Most Frequent Words in Low Ratings:")
        freq_words = self.extract_frequent_words('low', n=30)
        for i, (word, count) in enumerate(list(freq_words.items())[:20], 1):
            print(f"   {i:2d}. {word:20s} ({count:,})")

        # ë°©ë²• 2: TF-IDF (ì €í‰ì ì— íŠ¹ì§•ì ì¸ ë‹¨ì–´)
        print("\n2ï¸âƒ£  Distinctive Words in Low Ratings (TF-IDF):")
        tfidf_words = self.extract_distinctive_words_tfidf('low', n=20)
        for i, (word, score) in enumerate(tfidf_words, 1):
            print(f"   {i:2d}. {word:30s} (score: {score:.4f})")

        # ë°©ë²• 3: 2-gram (êµ¬ë¬¸ ë¶„ì„)
        print("\n3ï¸âƒ£  Common Phrases in Low Ratings (2-grams):")
        bigrams = self.extract_ngrams('low', n=2, top_k=20)
        for i, (phrase, count) in enumerate(list(bigrams.items())[:20], 1):
            print(f"   {i:2d}. '{phrase}' ({count:,})")

        # ë°©ë²• 4: ê·¸ë£¹ ê°„ ë¹„êµ
        print("\n4ï¸âƒ£  Words More Common in Low vs High Ratings:")
        distinctive = self.compare_groups(top_n=20)
        for i, (word, data) in enumerate(list(distinctive.items())[:20], 1):
            print(f"   {i:2d}. {word:20s} - Low: {data['low_count']:,}, High: {data['high_count']:,} (ratio: {data['ratio']:.1f}x)")

    def discover_success_factor_keywords(self):
        """
        Success Factor í‚¤ì›Œë“œ ìë™ ë°œê²¬
        """
        print("\n" + "="*80)
        print("SUCCESS FACTOR KEYWORD DISCOVERY (from 4-5 star reviews)")
        print("="*80)

        # ë¹ˆë„ ë¶„ì„
        print("\n1ï¸âƒ£  Most Frequent Words in High Ratings:")
        freq_words = self.extract_frequent_words('high', n=30)
        for i, (word, count) in enumerate(list(freq_words.items())[:20], 1):
            print(f"   {i:2d}. {word:20s} ({count:,})")

        # TF-IDF
        print("\n2ï¸âƒ£  Distinctive Words in High Ratings (TF-IDF):")
        tfidf_words = self.extract_distinctive_words_tfidf('high', n=20)
        for i, (word, score) in enumerate(tfidf_words, 1):
            print(f"   {i:2d}. {word:30s} (score: {score:.4f})")

        # 2-gram
        print("\n3ï¸âƒ£  Common Phrases in High Ratings (2-grams):")
        bigrams = self.extract_ngrams('high', n=2, top_k=20)
        for i, (phrase, count) in enumerate(list(bigrams.items())[:20], 1):
            print(f"   {i:2d}. '{phrase}' ({count:,})")

    def suggest_keyword_groups(self):
        """
        ë°œê²¬ëœ í‚¤ì›Œë“œë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™” ì œì•ˆ
        """
        print("\n" + "="*80)
        print("SUGGESTED KEYWORD GROUPINGS")
        print("="*80)

        low_words = self.extract_frequent_words('low', n=100)

        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ì œì•ˆ
        categories = {
            "premium_related": [],
            "data_related": [],
            "technical_issues": [],
            "ux_issues": [],
            "account_related": [],
            "feature_requests": []
        }

        # Premium/Payment ê´€ë ¨
        premium_patterns = ['premium', 'paid', 'pay', 'subscription', 'price', 'cost', 'expensive', 'money', 'free']
        categories['premium_related'] = [w for w in low_words if any(p in w for p in premium_patterns)]

        # Data/Loss ê´€ë ¨
        data_patterns = ['data', 'lost', 'delete', 'save', 'backup', 'sync', 'disappear', 'gone', 'missing']
        categories['data_related'] = [w for w in low_words if any(p in w for p in data_patterns)]

        # Technical ê´€ë ¨
        tech_patterns = ['crash', 'bug', 'glitch', 'error', 'broken', 'freeze', 'lag', 'slow']
        categories['technical_issues'] = [w for w in low_words if any(p in w for p in tech_patterns)]

        # UX ê´€ë ¨
        ux_patterns = ['confusing', 'complicated', 'difficult', 'hard', 'cant', 'wont', 'doesnt']
        categories['ux_issues'] = [w for w in low_words if any(p in w for p in ux_patterns)]

        # Account ê´€ë ¨
        account_patterns = ['account', 'login', 'password', 'sign']
        categories['account_related'] = [w for w in low_words if any(p in w for p in account_patterns)]

        for category, words in categories.items():
            if words:
                print(f"\nğŸ“¦ {category.replace('_', ' ').title()}:")
                print(f"   {', '.join(words[:10])}")

    def generate_keyword_config(self, output_path=None):
        """
        ë°œê²¬ëœ í‚¤ì›Œë“œë¥¼ ì„¤ì • íŒŒì¼ë¡œ ì €ì¥
        """
        config = {
            "pain_keywords": {},
            "feature_keywords": {},
            "sentiment_keywords": {}
        }

        # Pain points (ì €í‰ì ì—ì„œ íŠ¹ì§•ì ì¸ ë‹¨ì–´ë“¤)
        distinctive = self.compare_groups(top_n=100)

        # ìˆ˜ë™ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ì‹¤ì œë¡œëŠ” í´ëŸ¬ìŠ¤í„°ë§ì´ë‚˜ LLM ì‚¬ìš© ê°€ëŠ¥)
        pain_categories = {
            "premium": [],
            "data_loss": [],
            "bugs": [],
            "ux_issues": [],
            "account": []
        }

        for word in distinctive.keys():
            if any(p in word for p in ['premium', 'paid', 'pay', 'subscription', 'price', 'cost']):
                pain_categories['premium'].append(word)
            elif any(p in word for p in ['lost', 'delete', 'disappear', 'gone', 'missing', 'data']):
                pain_categories['data_loss'].append(word)
            elif any(p in word for p in ['crash', 'bug', 'glitch', 'error', 'broken', 'freeze']):
                pain_categories['bugs'].append(word)
            elif any(p in word for p in ['account', 'login', 'password', 'sign']):
                pain_categories['account'].append(word)
            elif any(p in word for p in ['confusing', 'complicated', 'difficult', 'hard']):
                pain_categories['ux_issues'].append(word)

        config['pain_keywords'] = pain_categories

        # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ
        high_words = self.extract_frequent_words('high', n=50)
        low_words = self.extract_frequent_words('low', n=50)

        positive_indicators = ['love', 'great', 'amazing', 'perfect', 'best', 'helpful', 'easy', 'simple']
        negative_indicators = ['hate', 'terrible', 'awful', 'worst', 'bad', 'useless']

        config['sentiment_keywords'] = {
            'positive': [w for w in high_words if any(p in w for p in positive_indicators)],
            'negative': [w for w in low_words if any(p in w for p in negative_indicators)]
        }

        if output_path:
            import json
            with open(output_path, 'w') as f:
                json.dump(config, f, indent=2)
            print(f"\nğŸ’¾ Keyword configuration saved to: {output_path}")

        return config


def main():
    csv_path = '/Users/hunjunsin/Desktop/Jun/hcai/FinalProject/reflecta/review-scraping/data/MHARD_dataset.csv'

    discoverer = KeywordDiscovery(csv_path)

    # Pain point í‚¤ì›Œë“œ ë°œê²¬
    discoverer.discover_pain_point_keywords()

    # Success factor í‚¤ì›Œë“œ ë°œê²¬
    discoverer.discover_success_factor_keywords()

    # í‚¤ì›Œë“œ ê·¸ë£¹í™” ì œì•ˆ
    discoverer.suggest_keyword_groups()

    # ì„¤ì • íŒŒì¼ ìƒì„±
    config_path = '/Users/hunjunsin/Desktop/Jun/hcai/FinalProject/reflecta/review-scraping/data/discovered_keywords.json'
    discoverer.generate_keyword_config(config_path)


if __name__ == "__main__":
    main()
